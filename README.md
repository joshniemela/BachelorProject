# BachelorProject
This project will explore the oversquashing problem in Graph Neural Networks (GNNs) and how it affects the ability for the model to learn long-range dependencies between nodes.
The project will also explore the use of topological information in the form of the dataset's graph structure to improve the model's ability to learn long-range dependencies.  

Supervised by: Raghavendra Selvan  

## Random thoughts
Attention is analogous to a fully-adjacent graph layer, therefore it is a special case of a GNN.
Therefore, it should be possible to find some trade-off between losing computing efficiency and performance.


<!--
This might need to be moved to its own file or compiled to LaTeX
-->
# Bachelor project - motivation
By: Joshua Victor Niemel√§ and Mustafa Hekmat Al-Abdelamir
## Introduction

## Data
Various synthetic datasets or real datasets

## Methods

## Learning Objectives
1. Understand the theory behind Graph Neural Networks (GNNs) and how they can be used to model graph-structured data.
2. Understand how topological information can be used to improve the performance of GNNs.
3. Construct a transformer and ordinary GNN model and compare their performance and training times.
4. Investigate the oversquashing problem in GNNs and how it affects the ability for the model to learn long-range dependencies between nodes.
5. Investigate various methods utilising graph rewiring or other non-destructive methods such as Graph Echo State Networks to improve the performance of GNNs on long-range dependencies.
